# 40-AI-model의 모델을 기반 RESTAPI 서비스 구축하기

### FASTAPI 기반
[FASTAPI 기반 가이드 문서 ](./fastapi/fastapi_model_serving_explanation.md)  
[FASTAPI 기반 정리폴더 ](./fastapi)


### vLLM 기반
[vLLM 기반 가이드문서 ](./vllm/vLLM_Conversion_Guide.md)  
[vLLM 기반 정리폴더 ](./vllm)



# vLLM 사용 코드와 PyTorch 기반 코드 비교
## 1. 모델 로드 방식

- **vLLM 사용 코드**:
  - vLLM의 `AsyncLLMEngine`을 사용하여 모델을 로드하고 GPU 노드를 활용해 추론합니다. 이로 인해 vLLM이 제공하는 최적화된 메모리 관리와 GPU 활용 기능을 간편하게 사용할 수 있습니다.

- **PyTorch 기반 코드**:
  - PyTorch로 학습된 `ABSAModel`을 직접 로드하고, 토크나이저와 인코더를 개별적으로 설정하여 추론합니다. 모든 디바이스 설정, 가중치 로드, 그리고 모델 설정 등을 수동으로 수행해야 합니다.

## 2. 코드의 복잡성

- **vLLM 사용 코드**:
  - 모델 로딩과 추론이 매우 간단합니다. vLLM 엔진을 초기화하고 바로 사용할 수 있어, 상대적으로 코드가 간결하고 사용이 용이합니다.

- **PyTorch 기반 코드**:
  - PyTorch 모델을 로드하는 과정에서 인코더 설정, 토크나이저 로드, 가중치 파일 로드, 디바이스 설정 등 다양한 세부적인 단계를 처리해야 합니다. 각 단계에서 오류가 발생할 가능성이 있으며 코드의 복잡성도 높습니다.

## 3. 추론 속도 및 최적화

- **vLLM 사용 코드**:
  - vLLM은 대규모 언어 모델(LLM)을 위한 최적화된 추론 엔진으로, 메모리 효율성 및 GPU 활용을 극대화하도록 설계되었습니다. 특히 병렬 요청 처리나 토큰 캐싱 등을 통해 추론 속도를 향상시킬 수 있습니다.

- **PyTorch 기반 코드**:
  - 일반적인 PyTorch 추론 방식은 대규모 추론에서 비효율적일 수 있습니다. GPU 최적화나 메모리 관리 기능이 vLLM에 비해 상대적으로 부족합니다.

## 4. 사용 편의성

- **vLLM 사용 코드**:
  - 모델 서빙을 위해 필요한 설정과 추론 관련 코드가 간단하여 사용하기가 용이합니다. GPU 노드 설정과 같은 복잡한 부분도 vLLM에서 자동으로 처리됩니다.

- **PyTorch 기반 코드**:
  - 모델 구성, 데이터 전처리, 후처리, 그리고 하드웨어 설정을 모두 직접 다루어야 하므로 설정과 유지 관리가 복잡할 수 있습니다.

## 5. 기능 차이

- **vLLM 사용 코드**:
  - vLLM의 경우 모델 추론만을 다루며, 모델 아키텍처나 구조를 커스터마이징하기 어렵습니다. 단순히 사전 학습된 모델을 서빙하고 추론하는 데 적합합니다.

- **PyTorch 기반 코드**:
  - `ABSAModel`이라는 커스텀 모델을 사용하여 여러 예측(감정, 속성 등)을 처리할 수 있습니다. 모델의 구조를 변경하거나 추가적인 커스터마이징이 필요할 때 PyTorch가 더 유연합니다.

## 요약

- **vLLM 코드**는 **간단하고 빠른 추론**에 중점을 둔 반면, **PyTorch 코드**는 **더 많은 커스터마이징과 세부 설정**이 가능하지만 그만큼 복잡합니다.
- **사용 편의성**과 **빠른 서빙**을 원할 경우에는 vLLM을, **세부 조정**이나 **모델 구조의 직접적인 수정**이 필요하다면 PyTorch 기반 코드가 더 적합할 수 있습니다.
